{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f142efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install non-standard dependencies for SageMaker\n",
    "!pip install -q geopandas shapely rasterio xarray ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f705b5",
   "metadata": {},
   "source": [
    "# SGS Foundational Concepts and Data Preprocessing\n",
    "\n",
    "Welcome to this training module on geospatial data analysis for mineral exploration. In this notebook, we'll cover the essential groundwork: loading different data types, understanding their formats, and preparing them for analysis.\n",
    "\n",
    "**What we'll cover:**\n",
    "- Different geospatial data formats (rasters vs. vectors)\n",
    "- Exploratory data analysis (EDA) for geochemical data\n",
    "- Data transformations and why they matter\n",
    "- Handling missing data\n",
    "- Understanding spatial autocorrelation and its implications\n",
    "\n",
    "Let's get started by configuring our data paths and loading the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79714cd9",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a538fc",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers as h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c3a12",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac7ec1",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "Before diving into any analysis, it's critical to define what you're trying to achieve. Ask yourself:\n",
    "\n",
    "- **What's the target?** (e.g., predicting prospectivity, finding anomalies, mapping alteration)\n",
    "- **What output do you need?** (e.g., a map, ranked target list, geochemical clusters)\n",
    "- **What constraints exist?** (data quality, resolution, budget for follow-up)\n",
    "\n",
    "Having a clear objective shapes every decision downstream—from which data to prioritize to which methods make sense.\n",
    "\n",
    "Some questions we will explore in the analytical methods notebook include:\n",
    "\n",
    "- *How can we better visualize spatial geochemical species abudance? `(IDW, Kriging)`*\n",
    "- *What are the dominant geochemical signatures, and which elements vary together? `(PCA)`*\n",
    "- *Can we group samples into distinct geochemical populations? `(K-means clustering)`*\n",
    "- *Which samples have unusual multi-element signatures worth investigating? `(Isolation Forest)`*\n",
    "- *Where are there dense concentrations of various minerals? `(Spectral classification)`*\n",
    "- *Can we predict prospectivity using known deposits? `(Supervised ML)`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b9e96",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd50112",
   "metadata": {},
   "source": [
    "## Load Helpers and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h.load_training_data()\n",
    "\n",
    "continuous_raster = data['continuous_raster']\n",
    "raster_extent = data['raster_extent']\n",
    "raster_crs = data['raster_crs']\n",
    "vector_gdf = data['vector_gdf']\n",
    "categorical_raster = data['categorical_raster']\n",
    "geochem_gdf = data['geochem_gdf']\n",
    "\n",
    "feature_cols, value_col = h.prepare_geochem_features(geochem_gdf)\n",
    "print('Data loaded and ready for preprocessing.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bfe8a7",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f423557",
   "metadata": {},
   "source": [
    "## Data Format Considerations\n",
    "\n",
    "Geospatial data comes in two main flavors:\n",
    "\n",
    "**Vector data** — discrete features with coordinates (points, lines, polygons)\n",
    "- *Lithology polygons*: boundaries of rock units mapped from fieldwork or remote sensing\n",
    "- *Geochem points*: sample locations with associated assay values\n",
    "\n",
    "**Raster data** — continuous grids of pixels, where each pixel holds a value\n",
    "- *Geophysics*: gridded magnetic, gravity, or radiometric surveys\n",
    "- *Spectral indices*: satellite-derived maps highlighting specific mineral signatures\n",
    "\n",
    "Understanding which format your data is in determines how you'll process and analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting data format examples...')\n",
    "_ = h.plot_data_format_examples(vector_gdf=vector_gdf, geochem_gdf=geochem_gdf)\n",
    "h.show_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759a1fe",
   "metadata": {},
   "source": [
    "The top two plots above represent the kinds of vector data we are ingesting - lithologic and geochemical. Both of these data layers contain attributes within them to describe each of the unique features (e.g. lithologic units, geochem samples).\n",
    "\n",
    "The bottom two plots represent two examples of the raster data we are ingesting - geophysical and spectral. Both are gridded, numeric datasets, where each pixel is assigned a value that relates to the original measurement (e.g. magnetic intensity, spectral reflectance).\n",
    "\n",
    "The ways in which we can use, interact with, and analyze these two kinds of data differs. Combining varied data formats such as these for ML processing can become quite complex - we will address this later on in our exercise when we reach the Prospectivity Mapping workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe1697",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182llrzbhv",
   "metadata": {},
   "source": [
    "## EDA, Transformations, and Scaling\n",
    "\n",
    "Exploratory Data Analysis (EDA) is your first look at the data. Before running any sophisticated models, you need to understand:\n",
    "- What does the distribution look like?\n",
    "- Are there outliers?\n",
    "- How do variables relate to each other?\n",
    "\n",
    "Let's start by getting a high-level summary of our geochemical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhcvoswu34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h.summarize_geochem(geochem_gdf, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ityuqxt68uf",
   "metadata": {},
   "source": [
    "This summary tells us:\n",
    "- **Dataset size:** How many samples and variables we're working with\n",
    "- **Element types:** Breakdown of major oxides vs. trace elements (important for understanding data scale differences)\n",
    "- **Spatial extent:** The geographic footprint of our sampling\n",
    "- **Missing data:** An initial view of data completeness\n",
    "\n",
    "Now let's look at individual variable distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2nbmjupoibj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER PARAMETERS: Quantile Clipping ---\n",
    "# Clip extreme values to reduce the influence of outliers on visualizations/analysis.\n",
    "# Set to None for no clipping, or a tuple like (0.01, 0.99) to clip the bottom 1% and top 1%.\n",
    "# More aggressive clipping (e.g., 0.05, 0.95) removes more outliers but may hide real anomalies.\n",
    "\n",
    "clip_quantiles = None  # Try: (0.01, 0.99) or (0.05, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing distributions for EDA...')\n",
    "values = geochem_gdf[value_col].values\n",
    "values = h.clip_quantiles(values, clip_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb46dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = h.plot_distribution(geochem_gdf[feature_cols],\n",
    "                      title='First 10 Distributions',\n",
    "                      show_stats=False,\n",
    "                      max_plots=10,\n",
    "                      ncols=5)\n",
    "h.show_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacwnxqwep",
   "metadata": {},
   "source": [
    "### Interpreting the first 10 distributions\n",
    "\n",
    "Rather than a single element, these 10 plots show the range of distribution shapes you will commonly see in geochemical data:\n",
    "\n",
    "- **SiO2_percent_xrf, Al2O3_percent_xrf, K2O_percent_xrf, Na2O_percent_icp:** Broad, near-normal shapes typical of major oxides, with moderate spread.\n",
    "- **TFe2O3_percent_xrf, CaO_percent_icp:** Right-skewed with long tails, indicating a few higher-value samples.\n",
    "- **MgO_percent_icp, LOI_percent_gr, TC_percent_irs:** Strongly right-skewed; most samples are low with a small number of enriched values.\n",
    "- **Re_ppb_icp:** Extremely concentrated near zero with a sharp tail, typical for trace elements near detection limits.\n",
    "\n",
    "Key takeaway: many geochemical variables are **non-normal** and **right-skewed**, so transformation (e.g., log) is often needed before modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ieby13bbznj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER PARAMETERS: Data Transformation ---\n",
    "# Choose how to transform skewed data before analysis.\n",
    "# 'none'  = no transformation (use raw values)\n",
    "# 'log1p' = log(1 + x), handles zeros safely and normalizes right-skewed distributions\n",
    "# Log transforms are common for geochemical data where values span orders of magnitude.\n",
    "\n",
    "transform = 'log1p'  # Options: 'none', 'log1p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6899fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_values = h.apply_transform(values, transform=transform)\n",
    "_ = h.plot_transformation_comparison(values, log_values, transform_name=transform)\n",
    "h.show_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4gjih1ku6al",
   "metadata": {},
   "source": [
    "### Why log transformation helps\n",
    "\n",
    "Compare the two distributions:\n",
    "\n",
    "**Original (left):** Heavily right-skewed with most values bunched at the low end\n",
    "\n",
    "**Log-transformed (right):** Much more symmetric, approaching a normal distribution\n",
    "\n",
    "After transformation:\n",
    "- Mean (2.63) and Median (2.71) are now nearly equal\n",
    "- The standard deviation dropped from 16.28 to 0.87\n",
    "- The spread is more uniform, making patterns easier to detect\n",
    "\n",
    "**Key point:** We use `log(1 + x)` (log1p) rather than `log(x)` to handle zero values safely. Many analytical methods—regression, PCA, clustering—perform better on normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f7d2d",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x7qa6xirf58",
   "metadata": {},
   "source": [
    "### Feature Correlations\n",
    "\n",
    "Before modeling, it's useful to examine how variables relate to each other. Highly correlated variables carry redundant information—understanding these relationships helps with feature selection and interpreting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gm8f20q2vn5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER PARAMETERS: Feature Scaling ---\n",
    "# Whether to standardize features (subtract mean, divide by std dev) before correlation analysis.\n",
    "# True  = scale features to mean=0, std=1 (recommended for comparing variables on different scales)\n",
    "# False = use raw values (correlations still work, but visualization may be harder to interpret)\n",
    "# Scaling is essential when variables have vastly different units (e.g., ppm vs. percent).\n",
    "\n",
    "scale_features = True  # Options: True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scaling features and examining correlations...')\n",
    "\n",
    "if scale_features:\n",
    "    scaled_df, scaler = h.scale_features(geochem_gdf, feature_cols)\n",
    "else:\n",
    "    scaled_df = geochem_gdf[feature_cols].copy()\n",
    "    scaler = None\n",
    "_ = h.plot_correlation_matrix(scaled_df, title='Scaled Feature Correlations', annot=False)\n",
    "h.show_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k2zk1kfld",
   "metadata": {},
   "source": [
    "### Reading the correlation matrix\n",
    "\n",
    "This triangular heatmap shows pairwise correlations between all geochemical variables:\n",
    "- **Red = positive correlation** (elements that increase together)\n",
    "- **Blue = negative correlation** (as one increases, the other decreases)\n",
    "- **White/neutral = no correlation**\n",
    "\n",
    "**Notable patterns to look for:**\n",
    "- Strong positive correlations among trace elements (red clusters) often indicate shared geochemical behavior or common mineral hosts\n",
    "- SiO₂ commonly shows negative correlation with many elements—typical of dilution effects in silica-rich rocks\n",
    "- LOI (Loss on Ignition) correlations reflect volatile-bearing minerals like clays and carbonates\n",
    "- Element pairs with very high correlation (|r| > 0.8) may be redundant for modeling purposes\n",
    "\n",
    "**Why this matters:** This informs feature selection—highly correlated variables carry redundant information. PCA (covered later) is one way to combine correlated variables into independent components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f0ffa",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5apczwt1rzn",
   "metadata": {},
   "source": [
    "## Missing Data and Imputation\n",
    "\n",
    "Real-world geochemical datasets almost always have missing values. Common causes include:\n",
    "- Samples not analyzed for certain elements (cost constraints)\n",
    "- Values below detection limits (sometimes indicated by negative values!)\n",
    "- Data entry errors or QA/QC failures\n",
    "- Different analytical packages used across campaigns\n",
    "\n",
    "Before analysis, we need to:\n",
    "1. **Understand the pattern** — Is missingness random, or is there structure?\n",
    "2. **Decide on a strategy** — Drop rows/columns, or fill in (impute) values?\n",
    "\n",
    "For this demo, we'll artificially inject missing data (0-10% per column, randomly) to illustrate the workflow. Then we'll apply mean imputation as a simple fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dj2nv7a1kgd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER PARAMETERS: Missing Data Simulation ---\n",
    "# For demonstration, we inject random missing values into the dataset.\n",
    "# Format: (min_rate, max_rate) - each column gets a random missing % within this range.\n",
    "# (0, 0.1)  = 0-10% missing per column (light missingness)\n",
    "# (0.1, 0.3) = 10-30% missing per column (moderate missingness)\n",
    "# (0.3, 0.5) = 30-50% missing per column (heavy missingness, may require dropping columns)\n",
    "\n",
    "missing_rate = (0, 0.1)  # Try: (0.1, 0.3) to see more severe missingness patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Injecting missing values for demo...')\n",
    "geochem_missing = h.add_missing_data(\n",
    "    geochem_gdf,\n",
    "    missing_pct=missing_rate,\n",
    "    columns=feature_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3282ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = h.plot_missing_data_pattern(geochem_missing[feature_cols], figsize=(14, 7))\n",
    "h.show_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veoik3qvr7e",
   "metadata": {},
   "source": [
    "### Understanding missing data patterns\n",
    "\n",
    "**Left plot (Missing % by Column):** Shows the percentage of missing values for each variable. Since we injected random amounts (0-10%) per column, you'll see variation across variables. The red dashed line at 50% marks a common threshold—columns above this are often dropped entirely.\n",
    "\n",
    "**Right plot (Missing Data Pattern):** Each row is a sample, each column is a variable. Dark cells = missing values. This visualization helps identify:\n",
    "- Whether missingness is random or structured\n",
    "- If certain samples are missing many values (row patterns)\n",
    "- If certain variables are systematically missing together (column patterns)\n",
    "\n",
    "In real data, you might find that PGE (platinum group element) analyses are missing because only some samples were sent for that expensive analysis—that's **not random** missingness and requires different handling.\n",
    "\n",
    "Now let's fill in these gaps using mean imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebzk8usnfh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER PARAMETERS: Imputation Strategy ---\n",
    "# Choose how to fill in missing values.\n",
    "# 'mean'          = replace missing with the column mean (simple, reduces variance)\n",
    "# 'median'        = replace missing with the column median (more robust to outliers)\n",
    "# 'most_frequent' = replace missing with the most common value (better for categorical-like data)\n",
    "# Mean/median are typical for continuous geochemical data; median is preferred if outliers exist.\n",
    "\n",
    "impute_strategy = 'mean'  # Options: 'mean', 'median', 'most_frequent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running {impute_strategy} imputation...\")\n",
    "imputed_values, imputer = h.impute_values(\n",
    "    geochem_missing[feature_cols],\n",
    "    strategy=impute_strategy,\n",
    ")\n",
    "print('Imputation complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585237cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = h.plot_imputation_comparison(\n",
    "    geochem_missing[value_col].values,\n",
    "    imputed_values[:, feature_cols.index(value_col)],\n",
    "    column_name=value_col,\n",
    ")\n",
    "h.show_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taui6yv9kt",
   "metadata": {},
   "source": [
    "### Imputation results\n",
    "\n",
    "The above histograms show Cu values before and after imputation:\n",
    "\n",
    "**Before (blue):** The distribution has its natural shape, with gaps where values were missing.\n",
    "\n",
    "**After (green):** All samples now have values. The overall distribution shape is preserved because mean imputation doesn't distort the central tendency.\n",
    "\n",
    "**Caveats:**\n",
    "- Mean imputation is simple but reduces variance (all imputed values are identical within a column)\n",
    "- For exploration targeting, this is often acceptable\n",
    "- More sophisticated methods (KNN imputation, iterative imputation) can preserve more of the data's natural variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e911f0",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851e5d1",
   "metadata": {},
   "source": [
    "## Bias and Data Leakage (Spatial Autocorrelation)\n",
    "\n",
    "Geologic data are often spatially autocorrelated, which inflates model performance when random train/test splits are used. Use spatially aware validation (e.g., block cross-validation or buffered splits) to reduce leakage.\n",
    "\n",
    "This is a critical understanding for correct application of ML to geospatial data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = h.plot_spatial_autocorrelation(geochem_gdf, values, title=f'Spatial Autocorrelation: {value_col}')\n",
    "h.show_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gipmcwyqv2r",
   "metadata": {},
   "source": [
    "### Interpreting spatial autocorrelation\n",
    "\n",
    "**Left (Spatial Distribution):** Cu values mapped at sample locations. You can see clustering of high values (yellow/green) in certain areas, particularly in the upper-center portion of the study area.\n",
    "\n",
    "**Right (Moran Scatterplot):** This diagnostic plot shows:\n",
    "- X-axis: each sample's standardized Cu value\n",
    "- Y-axis: the average Cu value of its neighbors (spatial lag)\n",
    "- The red line's slope (0.722) is a proxy for Moran's I\n",
    "\n",
    "**Interpretation:**\n",
    "- Moran's I = 0.722 indicates **strong positive spatial autocorrelation**\n",
    "- Points cluster in the upper-right (high values surrounded by high neighbors) and lower-left (low values surrounded by low neighbors)\n",
    "- Very few points in off-diagonal quadrants (which would indicate spatial outliers)\n",
    "\n",
    "**Bottom line:** This dataset has significant spatial structure. Any ML modeling should account for this to avoid overly optimistic performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ypy7f9qp5rj",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we covered the foundational steps of geospatial data analysis:\n",
    "\n",
    "1. **Data formats:** Vectors (points, polygons) vs. rasters (gridded data)—each requires different handling\n",
    "2. **EDA:** Always examine your data distributions before analysis\n",
    "3. **Transformations:** Log transforms help normalize skewed geochemical data\n",
    "4. **Correlations:** Understanding relationships between variables informs feature selection\n",
    "5. **Missing data:** Visualize patterns, then choose appropriate imputation strategies\n",
    "6. **Spatial autocorrelation:** Geologic data is inherently spatially structured—account for this in validation\n",
    "\n",
    "With clean, well-understood data in hand, we're ready to move on to analytical methods in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
