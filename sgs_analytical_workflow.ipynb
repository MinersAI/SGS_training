{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SGS Analytical Workflow Notebook\n",
        "\n",
        "This notebook walks through core analytical concepts for geologic and geospatial data. It is designed to run on real datasets if you provide file paths, or generate synthetic data when paths are left empty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and User Data Paths\n",
        "\n",
        "Provide file paths to your data files (GeoTIFF or GeoJSON). Leave any path empty to use synthetic data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# USER CONFIGURATION\n",
        "# =============================================================================\n",
        "DATA_CONFIG = {\n",
        "    # Rasters\n",
        "    \"continuous_raster_path\": None,  # GeoTIFF with continuous values\n",
        "    \"categorical_raster_path\": None,  # GeoTIFF with class labels\n",
        "\n",
        "    # Vectors\n",
        "    \"vector_path\": None,  # GeoJSON or GeoPackage\n",
        "    \"geochem_points_path\": None,  # GeoJSON with geochemistry points\n",
        "\n",
        "    # Spectral halo classification\n",
        "    \"spectral_indices_dir\": None,  # Folder of spectral index GeoTIFFs\n",
        "\n",
        "    # Prospectivity mapping\n",
        "    \"prospectivity_feature_rasters\": [],  # List of raster paths (GeoTIFF)\n",
        "    \"prospectivity_training_points_path\": None,  # GeoJSON with known deposits\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.transform import from_bounds\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from scipy.spatial import KDTree\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Helpers and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "sys.path.append(str(Path.cwd()))\n",
        "import helpers as h\n",
        "\n",
        "\n",
        "def load_raster(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        data = src.read(1)\n",
        "        extent = (src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top)\n",
        "        return data, extent, src.crs\n",
        "\n",
        "\n",
        "def load_vector(path):\n",
        "    gdf = gpd.read_file(path)\n",
        "    return gdf\n",
        "\n",
        "\n",
        "def ensure_xy(gdf):\n",
        "    if 'X' not in gdf.columns or 'Y' not in gdf.columns:\n",
        "        gdf = gdf.copy()\n",
        "        gdf['X'] = gdf.geometry.x\n",
        "        gdf['Y'] = gdf.geometry.y\n",
        "    return gdf\n",
        "\n",
        "\n",
        "# --- Continuous raster ---\n",
        "if DATA_CONFIG['continuous_raster_path']:\n",
        "    continuous_raster, raster_extent, raster_crs = load_raster(DATA_CONFIG['continuous_raster_path'])\n",
        "else:\n",
        "    continuous_raster, raster_extent = h.generate_synthetic_raster()\n",
        "    raster_crs = 'EPSG:32610'\n",
        "\n",
        "# --- Categorical raster ---\n",
        "if DATA_CONFIG['categorical_raster_path']:\n",
        "    categorical_raster, _, _ = load_raster(DATA_CONFIG['categorical_raster_path'])\n",
        "else:\n",
        "    # Simple synthetic categorical raster derived from continuous\n",
        "    quantiles = np.quantile(continuous_raster[~np.isnan(continuous_raster)], [0.25, 0.5, 0.75])\n",
        "    categorical_raster = np.digitize(continuous_raster, bins=quantiles)\n",
        "\n",
        "# --- Vector data ---\n",
        "if DATA_CONFIG['vector_path']:\n",
        "    vector_gdf = load_vector(DATA_CONFIG['vector_path'])\n",
        "else:\n",
        "    gdf_points, gdf_lines, gdf_polygons = h.generate_synthetic_vector_geometries()\n",
        "    vector_gdf = gdf_points\n",
        "\n",
        "# --- Geochemistry points ---\n",
        "if DATA_CONFIG['geochem_points_path']:\n",
        "    geochem_gdf = load_vector(DATA_CONFIG['geochem_points_path'])\n",
        "else:\n",
        "    geochem_gdf = h.generate_synthetic_geochemistry()\n",
        "\n",
        "geochem_gdf = ensure_xy(geochem_gdf)\n",
        "\n",
        "print('Raster shape:', continuous_raster.shape)\n",
        "print('Vector records:', len(vector_gdf))\n",
        "print('Geochem records:', len(geochem_gdf))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Formulation\n",
        "\n",
        "Define the analytical goal and decision context up front. Examples include:\n",
        "- Predicting prospectivity from multi-source geoscience data.\n",
        "- Detecting multivariate geochemical anomalies.\n",
        "- Interpolating a sparse sample grid to continuous surfaces.\n",
        "\n",
        "Clarify what constitutes a positive target (e.g., mineralization, alteration halo), what output format is required (map, ranked targets, clusters), and which constraints apply (data quality, spatial resolution, computational cost).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Format Considerations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Raster vs vector comparison\n",
        "h.plot_raster_vs_vector(continuous_raster, geochem_gdf, extent=raster_extent)\n",
        "plt.show()\n",
        "\n",
        "# Point vs line vs polygon (synthetic)\n",
        "if 'gdf_points' not in globals():\n",
        "    gdf_points, gdf_lines, gdf_polygons = h.generate_synthetic_vector_geometries()\n",
        "\n",
        "h.plot_geometry_types(gdf_points, gdf_lines, gdf_polygons)\n",
        "plt.show()\n",
        "\n",
        "# Continuous vs categorical rasters\n",
        "h.plot_continuous_vs_categorical(continuous_raster, categorical_raster, extent=raster_extent)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA, Transformations, and Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Identify numeric feature columns\n",
        "numeric_cols = geochem_gdf.select_dtypes(include=[np.number]).columns.tolist()\n",
        "feature_cols = [c for c in numeric_cols if c not in ['X', 'Y']]\n",
        "if not feature_cols:\n",
        "    raise ValueError('No numeric feature columns found in geochem data.')\n",
        "\n",
        "value_col = feature_cols[0]\n",
        "values = geochem_gdf[value_col].values\n",
        "\n",
        "# Distribution and log transform\n",
        "h.plot_distribution(values, title=f'Distribution: {value_col}')\n",
        "plt.show()\n",
        "\n",
        "log_values = np.log1p(values - np.nanmin(values) + 1)\n",
        "h.plot_transformation_comparison(values, log_values, transform_name='Log1p')\n",
        "plt.show()\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(geochem_gdf[feature_cols])\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=feature_cols)\n",
        "\n",
        "h.plot_correlation_matrix(scaled_df, title='Scaled Feature Correlations')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Data and Imputation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inject missing values for demonstration\n",
        "geochem_missing = h.add_missing_data(geochem_gdf, missing_pct=0.1, columns=feature_cols)\n",
        "\n",
        "h.plot_missing_data_pattern(geochem_missing[feature_cols])\n",
        "plt.show()\n",
        "\n",
        "# Mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "imputed_values = imputer.fit_transform(geochem_missing[feature_cols])\n",
        "\n",
        "h.plot_imputation_comparison(\n",
        "    geochem_missing[value_col].values,\n",
        "    imputed_values[:, feature_cols.index(value_col)],\n",
        "    column_name=value_col\n",
        ")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bias and Data Leakage (Spatial Autocorrelation)\n",
        "\n",
        "Geologic data are often spatially autocorrelated, which inflates model performance when random train/test splits are used. Use spatially aware validation (e.g., block cross-validation or buffered splits) to reduce leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "values = geochem_gdf[value_col].values\n",
        "h.plot_spatial_autocorrelation(geochem_gdf, values, title='Spatial Autocorrelation')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analytical Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpolation (IDW + Kriging)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare sample points and grid\n",
        "sample_coords = geochem_gdf[['X', 'Y']].values\n",
        "sample_values = geochem_gdf[value_col].values\n",
        "\n",
        "xmin, xmax, ymin, ymax = raster_extent\n",
        "grid_resolution = 60\n",
        "x_grid = np.linspace(xmin, xmax, grid_resolution)\n",
        "y_grid = np.linspace(ymin, ymax, grid_resolution)\n",
        "xx, yy = np.meshgrid(x_grid, y_grid)\n",
        "grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
        "\n",
        "# IDW interpolation\n",
        "\n",
        "def idw_interpolation(sample_coords, sample_values, grid_points, power=2, n_neighbors=12):\n",
        "    tree = KDTree(sample_coords)\n",
        "    distances, indices = tree.query(grid_points, k=n_neighbors)\n",
        "    distances = np.maximum(distances, 1e-10)\n",
        "    weights = 1 / (distances ** power)\n",
        "    weights_sum = weights.sum(axis=1, keepdims=True)\n",
        "    weights_normalized = weights / weights_sum\n",
        "    interpolated = np.sum(weights_normalized * sample_values[indices], axis=1)\n",
        "    return interpolated\n",
        "\n",
        "idw_pred = idw_interpolation(sample_coords, sample_values, grid_points, power=2)\n",
        "idw_grid = idw_pred.reshape(grid_resolution, grid_resolution)\n",
        "\n",
        "h.plot_interpolation_results(sample_coords, sample_values, idw_grid, raster_extent, method_name='IDW')\n",
        "plt.show()\n",
        "\n",
        "# Empirical semivariogram\n",
        "\n",
        "def compute_semivariogram(coords, values, n_lags=12, max_lag=None):\n",
        "    dist_matrix = cdist(coords, coords)\n",
        "    if max_lag is None:\n",
        "        max_lag = np.percentile(dist_matrix, 50)\n",
        "\n",
        "    lag_edges = np.linspace(0, max_lag, n_lags + 1)\n",
        "    lag_centers = (lag_edges[:-1] + lag_edges[1:]) / 2\n",
        "\n",
        "    semivariance = []\n",
        "    for i in range(n_lags):\n",
        "        mask = (dist_matrix > lag_edges[i]) & (dist_matrix <= lag_edges[i + 1])\n",
        "        if mask.sum() > 0:\n",
        "            ii, jj = np.where(mask)\n",
        "            sq_diff = (values[ii] - values[jj]) ** 2\n",
        "            semivariance.append(0.5 * np.mean(sq_diff))\n",
        "        else:\n",
        "            semivariance.append(np.nan)\n",
        "\n",
        "    return lag_centers, np.array(semivariance)\n",
        "\n",
        "lags, semivar = compute_semivariogram(sample_coords, sample_values)\n",
        "\n",
        "# Spherical variogram model\n",
        "\n",
        "def spherical_variogram(h, nugget, sill, range_param):\n",
        "    h = np.asarray(h)\n",
        "    result = np.zeros_like(h, dtype=float)\n",
        "\n",
        "    mask = h > 0\n",
        "    h_norm = h[mask] / range_param\n",
        "\n",
        "    within_range = h_norm <= 1\n",
        "    result_temp = np.zeros_like(h_norm)\n",
        "    result_temp[within_range] = nugget + (sill - nugget) * (\n",
        "        1.5 * h_norm[within_range] - 0.5 * h_norm[within_range] ** 3\n",
        "    )\n",
        "    result_temp[~within_range] = sill\n",
        "\n",
        "    result[mask] = result_temp\n",
        "    return result\n",
        "\n",
        "\n",
        "def fit_variogram(lags, semivar):\n",
        "    from scipy.optimize import curve_fit\n",
        "\n",
        "    valid = ~np.isnan(semivar)\n",
        "    lags_clean = lags[valid]\n",
        "    semivar_clean = semivar[valid]\n",
        "\n",
        "    if len(lags_clean) < 3:\n",
        "        return 0, np.max(semivar_clean), np.max(lags_clean)\n",
        "\n",
        "    nugget_init = semivar_clean[0] if semivar_clean[0] > 0 else 0\n",
        "    sill_init = np.max(semivar_clean)\n",
        "    range_init = np.max(lags_clean) / 2\n",
        "\n",
        "    try:\n",
        "        popt, _ = curve_fit(\n",
        "            spherical_variogram,\n",
        "            lags_clean,\n",
        "            semivar_clean,\n",
        "            p0=[nugget_init, sill_init, range_init],\n",
        "            bounds=([0, 0, 1], [sill_init, sill_init * 2, np.max(lags_clean) * 2]),\n",
        "            maxfev=5000,\n",
        "        )\n",
        "        return popt\n",
        "    except Exception:\n",
        "        return nugget_init, sill_init, range_init\n",
        "\n",
        "\n",
        "def ordinary_kriging(sample_coords, sample_values, grid_points, nugget, sill, range_param, n_neighbors=12):\n",
        "    tree = KDTree(sample_coords)\n",
        "    predictions = np.zeros(len(grid_points))\n",
        "    variances = np.zeros(len(grid_points))\n",
        "\n",
        "    for i, point in enumerate(grid_points):\n",
        "        distances, indices = tree.query(point, k=n_neighbors)\n",
        "\n",
        "        if np.min(distances) < 1e-10:\n",
        "            predictions[i] = sample_values[indices[0]]\n",
        "            variances[i] = 0\n",
        "            continue\n",
        "\n",
        "        local_coords = sample_coords[indices]\n",
        "        local_values = sample_values[indices]\n",
        "\n",
        "        n = len(local_coords)\n",
        "        K = np.zeros((n + 1, n + 1))\n",
        "\n",
        "        for j in range(n):\n",
        "            for k in range(n):\n",
        "                dist = np.linalg.norm(local_coords[j] - local_coords[k])\n",
        "                K[j, k] = sill - spherical_variogram(dist, nugget, sill, range_param)\n",
        "\n",
        "        K[n, :n] = 1\n",
        "        K[:n, n] = 1\n",
        "        K[n, n] = 0\n",
        "\n",
        "        k0 = np.zeros(n + 1)\n",
        "        for j in range(n):\n",
        "            dist = np.linalg.norm(local_coords[j] - point)\n",
        "            k0[j] = sill - spherical_variogram(dist, nugget, sill, range_param)\n",
        "        k0[n] = 1\n",
        "\n",
        "        try:\n",
        "            weights = np.linalg.solve(K, k0)\n",
        "            predictions[i] = np.dot(weights[:n], local_values)\n",
        "            variances[i] = sill - np.dot(weights[:n], k0[:n]) - weights[n]\n",
        "        except Exception:\n",
        "            w = 1 / (distances ** 2)\n",
        "            predictions[i] = np.sum(w * local_values) / np.sum(w)\n",
        "            variances[i] = np.var(local_values)\n",
        "\n",
        "    return predictions, np.maximum(variances, 0)\n",
        "\n",
        "nugget, sill, range_param = fit_variogram(lags, semivar)\n",
        "model_fit = spherical_variogram(lags, nugget, sill, range_param)\n",
        "\n",
        "h.plot_semivariogram(lags, semivar, model_fit=model_fit, title='Fitted Variogram')\n",
        "plt.show()\n",
        "\n",
        "kriging_pred, kriging_var = ordinary_kriging(\n",
        "    sample_coords, sample_values, grid_points, nugget, sill, range_param, n_neighbors=12\n",
        ")\n",
        "\n",
        "kriging_grid = kriging_pred.reshape(grid_resolution, grid_resolution)\n",
        "variance_grid = kriging_var.reshape(grid_resolution, grid_resolution)\n",
        "\n",
        "h.plot_interpolation_results(sample_coords, sample_values, kriging_grid, raster_extent, method_name='Ordinary Kriging')\n",
        "plt.show()\n",
        "\n",
        "h.plot_raster(variance_grid, title='Kriging Variance', extent=raster_extent, cmap='YlOrRd')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(geochem_gdf[feature_cols])\n",
        "\n",
        "pca = PCA(n_components=min(5, len(feature_cols)))\n",
        "_ = pca.fit_transform(scaled_features)\n",
        "\n",
        "h.plot_pca_results(pca, feature_cols)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-means (Geochemical Populations)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "k_range = range(2, 8)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_range:\n",
        "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = km.fit_predict(scaled_features)\n",
        "    inertias.append(km.inertia_)\n",
        "    silhouettes.append(silhouette_score(scaled_features, labels))\n",
        "\n",
        "h.plot_elbow_silhouette(list(k_range), inertias, silhouettes)\n",
        "plt.show()\n",
        "\n",
        "best_k = list(k_range)[int(np.argmax(silhouettes))]\n",
        "km = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
        "labels = km.fit_predict(scaled_features)\n",
        "\n",
        "pca_2 = PCA(n_components=2)\n",
        "pca_2_features = pca_2.fit_transform(scaled_features)\n",
        "\n",
        "centers_2d = pca_2.transform(km.cluster_centers_)\n",
        "\n",
        "h.plot_clustering_results(pca_2_features, labels, centers=centers_2d,\n",
        "                          title=f'K-means Clusters (k={best_k})', feature_x=0, feature_y=1)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multivariate Anomaly Detection (Isolation Forest)\n",
        "\n",
        "See full reference implementation in `anomaly_detection/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "iso = IsolationForest(n_estimators=200, contamination=0.05, random_state=42)\n",
        "iso_labels = iso.fit_predict(scaled_features)\n",
        "scores = -iso.decision_function(scaled_features)\n",
        "\n",
        "h.plot_anomaly_scores(geochem_gdf, scores, binary_labels=iso_labels, title='Isolation Forest')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spectral Halo Classification\n",
        "\n",
        "See full reference implementation in `MinersWork/spectral_unsupervised/spectral_unsupervised_class.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load spectral indices from disk if provided, otherwise create synthetic indices\n",
        "spectral_indices = {}\n",
        "\n",
        "if DATA_CONFIG['spectral_indices_dir']:\n",
        "    dir_path = Path(DATA_CONFIG['spectral_indices_dir'])\n",
        "    spectral_extent = None\n",
        "    for tif_path in dir_path.glob('*.tif'):\n",
        "        data, extent, _ = load_raster(tif_path)\n",
        "        spectral_indices[tif_path.stem] = data\n",
        "        if spectral_extent is None:\n",
        "            spectral_extent = extent\n",
        "    if spectral_extent is None:\n",
        "        spectral_extent = raster_extent\n",
        "else:\n",
        "    base = continuous_raster\n",
        "    spectral_indices = {\n",
        "        'NDVI': base + np.random.normal(0, 1.0, base.shape),\n",
        "        'NDMI': gaussian_filter(base, sigma=3),\n",
        "        'BSI': np.flipud(base) + np.random.normal(0, 1.0, base.shape),\n",
        "    }\n",
        "    spectral_extent = raster_extent\n",
        "\n",
        "# Compute smoothed presence maps\n",
        "presence_maps = []\n",
        "for name, arr in spectral_indices.items():\n",
        "    threshold = np.nanquantile(arr, 0.9)\n",
        "    presence = (arr >= threshold).astype(float)\n",
        "    smoothed = gaussian_filter(presence, sigma=5)\n",
        "    presence_maps.append(smoothed)\n",
        "\n",
        "stack = np.stack(presence_maps, axis=-1)\n",
        "stack_2d = stack.reshape(-1, stack.shape[-1])\n",
        "\n",
        "# K-means to classify halo types\n",
        "n_classes = 5\n",
        "km = KMeans(n_clusters=n_classes, random_state=42, n_init=10)\n",
        "labels = km.fit_predict(stack_2d)\n",
        "class_map = labels.reshape(stack.shape[0], stack.shape[1])\n",
        "\n",
        "h.plot_alteration_map(class_map)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised ML Prospectivity Mapping\n",
        "\n",
        "See full reference implementation in `supervised_ML/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Synthetic raster stack (or load from user paths)\n",
        "if DATA_CONFIG['prospectivity_feature_rasters']:\n",
        "    feature_stack = []\n",
        "    for path in DATA_CONFIG['prospectivity_feature_rasters']:\n",
        "        data, extent, _ = load_raster(path)\n",
        "        feature_stack.append(data)\n",
        "    prospect_extent = extent\n",
        "    feature_stack = np.stack(feature_stack, axis=-1)\n",
        "else:\n",
        "    r1 = continuous_raster\n",
        "    r2 = gaussian_filter(continuous_raster, sigma=4)\n",
        "    r3 = np.flipud(continuous_raster)\n",
        "    feature_stack = np.stack([r1, r2, r3], axis=-1)\n",
        "    prospect_extent = raster_extent\n",
        "\n",
        "rows, cols, n_features = feature_stack.shape\n",
        "\n",
        "# Training data\n",
        "if DATA_CONFIG['prospectivity_training_points_path']:\n",
        "    train_gdf = load_vector(DATA_CONFIG['prospectivity_training_points_path'])\n",
        "    label_candidates = [c for c in train_gdf.columns if c.lower() in ['label', 'target', 'prospectivity']]\n",
        "    if not label_candidates:\n",
        "        raise ValueError('Training points file must include a label column like label/target/prospectivity.')\n",
        "    label_col = label_candidates[0]\n",
        "\n",
        "    train_gdf = ensure_xy(train_gdf)\n",
        "    xmin, xmax, ymin, ymax = prospect_extent\n",
        "\n",
        "    col_idx = ((train_gdf['X'] - xmin) / (xmax - xmin) * (cols - 1)).astype(int)\n",
        "    row_idx = ((ymax - train_gdf['Y']) / (ymax - ymin) * (rows - 1)).astype(int)\n",
        "    valid = (col_idx >= 0) & (col_idx < cols) & (row_idx >= 0) & (row_idx < rows)\n",
        "\n",
        "    X = feature_stack[row_idx[valid], col_idx[valid], :]\n",
        "    y = train_gdf.loc[valid, label_col].values\n",
        "else:\n",
        "    # Synthetic labels from a high-value anomaly\n",
        "    label_raster = (feature_stack[..., 0] > np.nanpercentile(feature_stack[..., 0], 92)).astype(int)\n",
        "\n",
        "    n_samples = 2000\n",
        "    rng = np.random.default_rng(42)\n",
        "    row_idx = rng.integers(0, rows, n_samples)\n",
        "    col_idx = rng.integers(0, cols, n_samples)\n",
        "\n",
        "    X = feature_stack[row_idx, col_idx, :]\n",
        "    y = label_raster[row_idx, col_idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict prospectivity across the grid\n",
        "flat_features = feature_stack.reshape(-1, n_features)\n",
        "prob = rf.predict_proba(flat_features)[:, 1].reshape(rows, cols)\n",
        "\n",
        "h.plot_prospectivity_map(prob, extent=prospect_extent, threshold=0.6)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, rf.predict(X_test)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}